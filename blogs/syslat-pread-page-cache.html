<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>How Long Does pread() Really Take?</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<h1>How Long Does <code>pread()</code> Really Take?</h1>

<p>
At some point, every performance investigation collapses into a deceptively
simple question:
</p>

<blockquote>
<p>
“How long does <em>this</em> actually take?”
</p>
</blockquote>

<p>
This page documents one such question, stripped down as far as I know how
to strip it:
</p>

<blockquote>
<p>
How long does a single <code>pread()</code> system call take, and what role
does the page cache play?
</p>
</blockquote>

<p>
Not throughput. Not benchmarks. Not “real-world workloads”.
Just one syscall, measured carefully.
</p>

<p>
The code for this experiment lives here:
</p>

<p>
<a href="https://github.com/Emmanuel326/syslat">
https://github.com/Emmanuel326/syslat
</a>
</p>

<hr>

<h2>Why ask such a small question?</h2>

<p>
Most performance discussions mix several effects into one number:
syscall overhead, page faults, storage latency, filesystem behavior,
CPU scheduling, and sometimes sheer luck.
</p>

<p>
Once mixed, cause and effect become hard to separate.
</p>

<p>
Rather than arguing about numbers, this experiment backs up and asks
a deliberately narrow question under explicit conditions.
</p>

<p>
If we can’t reason about one syscall in isolation,
we have no business reasoning about systems built on top of thousands of them.
</p>

<hr>

<h2>What is actually being measured</h2>

<p>
Each data point is the elapsed time of one <code>pread()</code> call,
measured from userspace entry to userspace return.
</p>

<p>
Each iteration:
</p>

<ul>
  <li>enters the kernel</li>
  <li>executes <code>pread()</code></li>
  <li>returns to userspace</li>
  <li>records elapsed time in nanoseconds</li>
</ul>

<p>
There is no averaging, no warm-up phase, and no filtering.
Every syscall stands on its own.
</p>

<p>
The output is intentionally raw.
Interpretation is a separate step.
</p>

<hr>

<h2>The shape of the experiment</h2>

<h3>Binary shape</h3>

<ul>
  <li>Single executable</li>
  <li>Single source file</li>
  <li>Single thread</li>
  <li>No frameworks, helpers, or abstractions</li>
</ul>

<p>
If the experiment does not fit in one file,
it is not yet simple enough to trust.
</p>

<h3>Execution model</h3>

<ul>
  <li>One process</li>
  <li>CPU affinity pinned externally via <code>taskset</code></li>
  <li>No CPU migration during measurement</li>
</ul>

<p>
CPU pinning is treated as environmental control,
not part of the program itself.
</p>

<hr>

<h2>Cold cache vs warm cache</h2>

<p>
Page cache state is treated as an initial condition,
not a runtime toggle.
</p>

<h3>Cold runs</h3>

<pre>
sync
echo 3 | sudo tee /proc/sys/vm/drop_caches
</pre>

<p>
This forces the page cache to start empty.
Early iterations will include page faults and real I/O.
</p>

<h3>Warm runs</h3>

<pre>
cat testfile &gt; /dev/null
</pre>

<p>
This ensures the file is resident in the page cache before measurement.
</p>

<p>
The binary itself is unchanged between runs.
Only the initial conditions differ.
</p>

<hr>

<h2>Why <code>pread()</code>?</h2>

<p>
<code>pread()</code> avoids shared file offset state.
Each iteration reads the same offset and follows the same kernel path.
</p>

<p>
This removes <code>lseek()</code> noise and keeps the syscall behavior
as consistent as possible across iterations.
</p>

<p>
The goal is not realism, but repeatability.
</p>

<hr>

<h2>What this experiment is not</h2>

<ul>
  <li>It is not measuring filesystem throughput</li>
  <li>It is not benchmarking NVMe devices</li>
  <li>It is not measuring scheduler or interrupt latency</li>
  <li>It is not claiming to represent “application performance”</li>
</ul>

<p>
Those are all valid questions.
They just require different experiments.
</p>

<hr>

<h2>Output, by design</h2>

<p>
The program emits one number per line:
</p>

<pre>
latency_ns
latency_ns
latency_ns
</pre>

<p>
There are no summaries and no opinions in the output.
The measurement path is kept as short and transparent as possible.
</p>

<p>
Any aggregation or visualization happens after the fact.
</p>

<hr>

<h2>Where this goes next</h2>

<p>
This page documents the baseline.
</p>

<p>
Future work will modify exactly one variable at a time —
block size, offset patterns, storage media, CPU isolation —
while preserving the existing structure.
</p>

<p>
As the experiment evolves, this page will grow with it.
</p>

</body>
</html>

